# ä¸Šäº¿çº§è¯å•æ•°æ®ä¼˜åŒ–æ–¹æ¡ˆ

## ğŸ“Š é—®é¢˜åˆ†æ

### æ•°æ®è§„æ¨¡é¢„ä¼°
- **æ—¥å‡è¯å•**: å‡è®¾100ä¸‡æ¡/å¤©
- **å¹´åº¦æ€»é‡**: 3.65äº¿æ¡
- **3å¹´ç´¯è®¡**: è¶…è¿‡10äº¿æ¡
- **å•æ¡å¤§å°**: çº¦1-2KBï¼ˆå«rawå­—æ®µï¼‰
- **æ€»å­˜å‚¨**: 1TB+

### é¢ä¸´çš„æŒ‘æˆ˜
1. âŒ æŸ¥è¯¢æ€§èƒ½ä¸‹é™ï¼ˆå…¨è¡¨æ‰«æï¼‰
2. âŒ æ’å…¥é€Ÿåº¦å˜æ…¢
3. âŒ ç´¢å¼•ä½“ç§¯è¿‡å¤§
4. âŒ å¤‡ä»½å’Œæ¢å¤å›°éš¾
5. âŒ ç£ç›˜ç©ºé—´å‹åŠ›

---

## ğŸ¯ ä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: æ•°æ®åº“åˆ†åŒºè¡¨ï¼ˆPostgreSQL Partitioningï¼‰â˜…â˜…â˜…â˜…â˜…

#### 1.1 æŒ‰æœˆåˆ†åŒºï¼ˆæ¨èï¼‰

```sql
-- åˆ›å»ºåˆ†åŒºä¸»è¡¨
CREATE TABLE cdrs_partitioned (
    id BIGSERIAL,
    vos_id INTEGER NOT NULL,
    caller VARCHAR(64),
    callee VARCHAR(64),
    caller_gateway VARCHAR(64),
    callee_gateway VARCHAR(64),
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP,
    duration INTEGER,
    cost NUMERIC(10,4),
    disposition VARCHAR(64),
    raw JSONB,  -- ä½¿ç”¨JSONBæ›¿ä»£TEXTï¼Œæ”¯æŒJSONæŸ¥è¯¢
    hash VARCHAR(32),
    created_at TIMESTAMP DEFAULT NOW()
) PARTITION BY RANGE (start_time);

-- åˆ›å»ºåˆ†åŒºï¼ˆæŒ‰æœˆï¼‰
CREATE TABLE cdrs_2025_01 PARTITION OF cdrs_partitioned
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE cdrs_2025_02 PARTITION OF cdrs_partitioned
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');

-- ... ç»§ç»­åˆ›å»ºæ›´å¤šåˆ†åŒº

-- åˆ›å»ºç´¢å¼•ï¼ˆè‡ªåŠ¨åº”ç”¨åˆ°æ‰€æœ‰åˆ†åŒºï¼‰
CREATE INDEX idx_cdrs_part_vos_time ON cdrs_partitioned(vos_id, start_time);
CREATE INDEX idx_cdrs_part_caller ON cdrs_partitioned(caller);
CREATE INDEX idx_cdrs_part_callee ON cdrs_partitioned(callee);
CREATE INDEX idx_cdrs_part_hash ON cdrs_partitioned(vos_id, hash);
```

**ä¼˜ç‚¹**:
- âœ… æŸ¥è¯¢è‡ªåŠ¨å®šä½åˆ°ç‰¹å®šåˆ†åŒºï¼ˆæŸ¥è¯¢é€Ÿåº¦æå‡10-100å€ï¼‰
- âœ… åˆ é™¤è€æ•°æ®åªéœ€DROPåˆ†åŒºï¼ˆç§’çº§å®Œæˆï¼‰
- âœ… ç»´æŠ¤å’Œå¤‡ä»½å¯æŒ‰åˆ†åŒºè¿›è¡Œ
- âœ… ç´¢å¼•ä½“ç§¯åˆ†æ•£ï¼Œç»´æŠ¤æˆæœ¬ä½

**å®ç°æ­¥éª¤**:
1. åˆ›å»ºæ–°çš„åˆ†åŒºè¡¨ç»“æ„
2. æ•°æ®è¿ç§»ï¼ˆåˆ†æ‰¹è¿›è¡Œï¼‰
3. ä¿®æ”¹ORMæ¨¡å‹
4. åˆ›å»ºè‡ªåŠ¨åˆ†åŒºç®¡ç†è„šæœ¬

#### 1.2 è‡ªåŠ¨åˆ†åŒºç®¡ç†è„šæœ¬

```python
# backend/app/scripts/partition_manager.py
from datetime import datetime, timedelta
from app.core.db import SessionLocal

def create_next_month_partition():
    """è‡ªåŠ¨åˆ›å»ºä¸‹ä¸ªæœˆçš„åˆ†åŒº"""
    db = SessionLocal()
    try:
        next_month = datetime.now() + timedelta(days=32)
        partition_name = f"cdrs_{next_month.strftime('%Y_%m')}"
        start_date = next_month.replace(day=1)
        end_date = (start_date + timedelta(days=32)).replace(day=1)
        
        sql = f"""
        CREATE TABLE IF NOT EXISTS {partition_name} 
        PARTITION OF cdrs_partitioned
        FOR VALUES FROM ('{start_date.date()}') TO ('{end_date.date()}');
        """
        
        db.execute(sql)
        db.commit()
        print(f"âœ… Created partition: {partition_name}")
    except Exception as e:
        print(f"âŒ Failed to create partition: {e}")
    finally:
        db.close()

def cleanup_old_partitions(keep_months=12):
    """åˆ é™¤è¶…è¿‡æŒ‡å®šæœˆä»½çš„æ—§åˆ†åŒº"""
    db = SessionLocal()
    try:
        cutoff_date = datetime.now() - timedelta(days=keep_months*30)
        partition_name = f"cdrs_{cutoff_date.strftime('%Y_%m')}"
        
        # å…ˆå¯¼å‡ºåˆ°å½’æ¡£è¡¨
        db.execute(f"""
            CREATE TABLE IF NOT EXISTS cdrs_archive_{partition_name} 
            AS SELECT * FROM {partition_name};
        """)
        
        # åˆ é™¤åˆ†åŒº
        db.execute(f"DROP TABLE {partition_name};")
        db.commit()
        print(f"âœ… Archived and dropped partition: {partition_name}")
    except Exception as e:
        print(f"âŒ Failed to cleanup partition: {e}")
    finally:
        db.close()
```

---

### æ–¹æ¡ˆ2: æ•°æ®å½’æ¡£ç­–ç•¥ â˜…â˜…â˜…â˜…â˜…

#### 2.1 å†·çƒ­æ•°æ®åˆ†ç¦»

```python
# çƒ­æ•°æ®ï¼šæœ€è¿‘3ä¸ªæœˆï¼ˆåœ¨ä¸»è¡¨ï¼‰
# æ¸©æ•°æ®ï¼š3-12ä¸ªæœˆï¼ˆåœ¨åˆ†åŒºè¡¨ï¼‰
# å†·æ•°æ®ï¼š12ä¸ªæœˆä»¥ä¸Šï¼ˆå½’æ¡£åˆ°ç‹¬ç«‹è¡¨æˆ–å¯¹è±¡å­˜å‚¨ï¼‰

class CDRArchiveStrategy:
    """è¯å•å½’æ¡£ç­–ç•¥"""
    
    HOT_DATA_MONTHS = 3    # çƒ­æ•°æ®ä¿ç•™3ä¸ªæœˆ
    WARM_DATA_MONTHS = 12  # æ¸©æ•°æ®ä¿ç•™12ä¸ªæœˆ
    
    @staticmethod
    def archive_to_cold_storage(months_ago=12):
        """
        å½’æ¡£å†·æ•°æ®åˆ°å¯¹è±¡å­˜å‚¨ï¼ˆå¦‚MinIO/S3ï¼‰
        """
        db = SessionLocal()
        try:
            cutoff_date = datetime.now() - timedelta(days=months_ago*30)
            
            # 1. å¯¼å‡ºä¸ºParquetæ ¼å¼ï¼ˆå‹ç¼©æ¯”é«˜ï¼‰
            query = f"""
                COPY (
                    SELECT * FROM cdrs 
                    WHERE start_time < '{cutoff_date}'
                ) TO PROGRAM 'gzip > /backup/cdrs_{cutoff_date.strftime('%Y%m')}.csv.gz' 
                WITH CSV HEADER;
            """
            db.execute(query)
            
            # 2. åˆ é™¤å·²å½’æ¡£æ•°æ®
            db.execute(f"DELETE FROM cdrs WHERE start_time < '{cutoff_date}'")
            db.commit()
            
            # 3. ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨ï¼ˆMinIO/S3ï¼‰
            # upload_to_s3(f'/backup/cdrs_{cutoff_date.strftime('%Y%m')}.csv.gz')
            
            print(f"âœ… Archived data before {cutoff_date}")
        except Exception as e:
            db.rollback()
            print(f"âŒ Archive failed: {e}")
        finally:
            db.close()
```

#### 2.2 å½’æ¡£è¡¨ç»“æ„

```sql
-- åˆ›å»ºå½’æ¡£è¡¨ï¼ˆä»…ä¿ç•™å…³é”®å­—æ®µï¼Œå»é™¤rawï¼‰
CREATE TABLE cdrs_archive (
    id BIGINT,
    vos_id INTEGER,
    caller VARCHAR(64),
    callee VARCHAR(64),
    start_time TIMESTAMP,
    duration INTEGER,
    cost NUMERIC(10,4),
    archived_at TIMESTAMP DEFAULT NOW()
) PARTITION BY RANGE (start_time);

-- å‹ç¼©å½’æ¡£è¡¨
ALTER TABLE cdrs_archive SET (
    autovacuum_enabled = false,
    toast_tuple_target = 8160  -- æœ€å¤§å‹ç¼©
);
```

---

### æ–¹æ¡ˆ3: åŒæ­¥ç­–ç•¥ä¼˜åŒ– â˜…â˜…â˜…â˜…â˜…

#### 3.1 é™åˆ¶åŒæ­¥èŒƒå›´

```python
# backend/app/tasks/initial_sync_tasks.py

@celery.task
def initial_sync_for_new_instance(instance_id: int, days=7):
    """
    æ–°å¢VOSèŠ‚ç‚¹åˆå§‹åŒ–åŒæ­¥
    
    Args:
        instance_id: VOSå®ä¾‹ID
        days: åŒæ­¥æœ€è¿‘å¤šå°‘å¤©ï¼ˆé»˜è®¤7å¤©ï¼Œä¸è¦è¶…è¿‡30å¤©ï¼‰
    """
    # é™åˆ¶æœ€å¤§åŒæ­¥å¤©æ•°
    if days > 30:
        days = 30
        logger.warning(f"åŒæ­¥å¤©æ•°é™åˆ¶ä¸º30å¤©ï¼Œé¿å…æ•°æ®é‡è¿‡å¤§")
    
    # ... åŒæ­¥é€»è¾‘
```

#### 3.2 åˆ†é¡µæ‰¹é‡å¤„ç†

```python
@celery.task
def sync_cdrs_for_single_day_batch(instance_id: int, date_str: str):
    """
    åˆ†é¡µåŒæ­¥å•å¤©è¯å•ï¼ˆé¿å…ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤šæ•°æ®ï¼‰
    """
    PAGE_SIZE = 1000  # æ¯æ¬¡å¤„ç†1000æ¡
    page = 0
    
    while True:
        # åˆ†é¡µè¯·æ±‚VOS API
        result = client.call_api('/external/server/GetCdr', {
            'beginTime': date_str,
            'endTime': date_str,
            'pageNo': page,
            'pageSize': PAGE_SIZE
        })
        
        cdrs = result.get('infoCdrs', [])
        if not cdrs:
            break
        
        # æ‰¹é‡æ’å…¥ï¼ˆä½¿ç”¨bulk_insert_mappingsï¼‰
        db.bulk_insert_mappings(CDR, [
            {
                'vos_id': instance_id,
                'caller': cdr['callerE164'],
                'callee': cdr['calleeE164'],
                # ... å…¶ä»–å­—æ®µ
            }
            for cdr in cdrs
        ])
        db.commit()
        
        page += 1
        time.sleep(0.5)  # é¿å…APIé™æµ
```

#### 3.3 å¢é‡åŒæ­¥

```python
@celery.task
def incremental_sync_cdrs(instance_id: int):
    """
    å¢é‡åŒæ­¥ï¼šåªåŒæ­¥æœ€åä¸€æ¬¡åŒæ­¥æ—¶é—´ä¹‹åçš„æ•°æ®
    """
    db = SessionLocal()
    try:
        # è·å–æœ€ååŒæ­¥æ—¶é—´
        last_cdr = db.query(CDR).filter(
            CDR.vos_id == instance_id
        ).order_by(CDR.start_time.desc()).first()
        
        if last_cdr:
            begin_time = last_cdr.start_time.strftime('%Y%m%d')
        else:
            # é¦–æ¬¡åŒæ­¥ï¼ŒåªåŒæ­¥æœ€è¿‘7å¤©
            begin_time = (datetime.now() - timedelta(days=7)).strftime('%Y%m%d')
        
        end_time = datetime.now().strftime('%Y%m%d')
        
        # åŒæ­¥å¢é‡æ•°æ®
        sync_cdrs_by_date_range(instance_id, begin_time, end_time)
        
    finally:
        db.close()
```

---

### æ–¹æ¡ˆ4: æŸ¥è¯¢ä¼˜åŒ– â˜…â˜…â˜…â˜…â˜…

#### 4.1 å¼ºåˆ¶æ—¶é—´èŒƒå›´é™åˆ¶

```python
# backend/app/routers/cdr.py

@router.post('/instances/{instance_id}/cdrs/query')
async def query_cdrs_from_vos(
    instance_id: int,
    query_params: CDRQueryRequest,
    current_user: Annotated[User, Depends(get_current_user)],
    db: Session = Depends(get_db),
    force_vos: bool = Query(False)
):
    """æŸ¥è¯¢è¯å•ï¼ˆå¼ºåˆ¶æ—¶é—´èŒƒå›´é™åˆ¶ï¼‰"""
    
    # è§£ææ—¶é—´èŒƒå›´
    begin_date = datetime.strptime(query_params.begin_time, '%Y%m%d')
    end_date = datetime.strptime(query_params.end_time, '%Y%m%d')
    
    # é™åˆ¶æŸ¥è¯¢èŒƒå›´ä¸è¶…è¿‡31å¤©
    if (end_date - begin_date).days > 31:
        raise HTTPException(
            status_code=400, 
            detail='æŸ¥è¯¢èŒƒå›´ä¸èƒ½è¶…è¿‡31å¤©ï¼Œé¿å…æ€§èƒ½é—®é¢˜'
        )
    
    # é™åˆ¶ä¸èƒ½æŸ¥è¯¢è¶…è¿‡1å¹´å‰çš„æ•°æ®ï¼ˆå¼•å¯¼ç”¨æˆ·ä½¿ç”¨å½’æ¡£æ•°æ®ï¼‰
    if begin_date < datetime.now() - timedelta(days=365):
        raise HTTPException(
            status_code=400,
            detail='æŸ¥è¯¢1å¹´å‰çš„æ•°æ®è¯·è”ç³»ç®¡ç†å‘˜è®¿é—®å½’æ¡£ç³»ç»Ÿ'
        )
    
    # ... æŸ¥è¯¢é€»è¾‘
```

#### 4.2 ç»“æœé›†åˆ†é¡µ

```python
@router.post('/instances/{instance_id}/cdrs/query')
async def query_cdrs_from_vos(
    # ... å‚æ•°
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100)  # é™åˆ¶æœ€å¤§100æ¡/é¡µ
):
    """æŸ¥è¯¢è¯å•ï¼ˆå¸¦åˆ†é¡µï¼‰"""
    
    offset = (page - 1) * page_size
    
    # æŸ¥è¯¢æ€»æ•°ï¼ˆä½¿ç”¨countä¼˜åŒ–ï¼‰
    total = db.query(func.count(CDR.id)).filter(
        CDR.vos_id == instance_id,
        CDR.start_time >= begin_date,
        CDR.start_time <= end_date
    ).scalar()
    
    # æŸ¥è¯¢åˆ†é¡µæ•°æ®
    cdrs = db.query(CDR).filter(
        CDR.vos_id == instance_id,
        CDR.start_time >= begin_date,
        CDR.start_time <= end_date
    ).order_by(CDR.start_time.desc()).offset(offset).limit(page_size).all()
    
    return {
        'data': cdrs,
        'total': total,
        'page': page,
        'page_size': page_size,
        'total_pages': (total + page_size - 1) // page_size
    }
```

#### 4.3 æŸ¥è¯¢ç»“æœç¼“å­˜

```python
from functools import lru_cache
from redis import Redis

redis_client = Redis(host='redis', port=6379, db=1)

def get_cdr_query_cache(cache_key: str):
    """ä»Redisè·å–æŸ¥è¯¢ç¼“å­˜"""
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    return None

def set_cdr_query_cache(cache_key: str, data: dict, ttl=300):
    """è®¾ç½®æŸ¥è¯¢ç¼“å­˜ï¼ˆ5åˆ†é’Ÿï¼‰"""
    redis_client.setex(cache_key, ttl, json.dumps(data))
```

---

### æ–¹æ¡ˆ5: æ•°æ®åº“å‚æ•°ä¼˜åŒ– â˜…â˜…â˜…â˜…

#### 5.1 PostgreSQLé…ç½®ä¼˜åŒ–

```conf
# postgresql.conf

# å†…å­˜é…ç½®ï¼ˆå‡è®¾16GBæœåŠ¡å™¨ï¼‰
shared_buffers = 4GB              # å…±äº«ç¼“å†²åŒº
effective_cache_size = 12GB       # æœ‰æ•ˆç¼“å­˜å¤§å°
work_mem = 256MB                  # å•æ¬¡æ“ä½œå†…å­˜
maintenance_work_mem = 1GB        # ç»´æŠ¤æ“ä½œå†…å­˜

# å†™å…¥ä¼˜åŒ–
wal_buffers = 16MB
checkpoint_timeout = 15min
max_wal_size = 4GB
checkpoint_completion_target = 0.9

# æŸ¥è¯¢ä¼˜åŒ–
random_page_cost = 1.1            # SSDä¼˜åŒ–
effective_io_concurrency = 200    # SSDå¹¶å‘
max_worker_processes = 8
max_parallel_workers_per_gather = 4
max_parallel_workers = 8

# æ—¥å¿—
log_min_duration_statement = 1000  # è®°å½•æ…¢æŸ¥è¯¢ï¼ˆ>1sï¼‰
```

#### 5.2 å®šæœŸç»´æŠ¤

```sql
-- å®šæœŸVACUUMå’ŒANALYZEï¼ˆå‘¨æœ«æ‰§è¡Œï¼‰
VACUUM ANALYZE cdrs;

-- é‡å»ºç´¢å¼•ï¼ˆæœˆåº¦æ‰§è¡Œï¼‰
REINDEX TABLE cdrs;

-- æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
ANALYZE cdrs;
```

---

### æ–¹æ¡ˆ6: rawå­—æ®µä¼˜åŒ– â˜…â˜…â˜…â˜…

#### 6.1 ä½¿ç”¨JSONBæ›¿ä»£TEXT

```python
# backend/app/models/cdr.py

from sqlalchemy.dialects.postgresql import JSONB

class CDR(Base):
    __tablename__ = 'cdrs'
    
    # ... å…¶ä»–å­—æ®µ
    
    # ä½¿ç”¨JSONBæ›¿ä»£TEXT
    raw = Column(JSONB)  # æ”¯æŒJSONæŸ¥è¯¢ï¼Œè‡ªåŠ¨å‹ç¼©
```

**ä¼˜ç‚¹**:
- âœ… è‡ªåŠ¨å‹ç¼©ï¼ŒèŠ‚çœ30-50%ç©ºé—´
- âœ… æ”¯æŒJSONå­—æ®µæŸ¥è¯¢ï¼š`raw->>'caller'`
- âœ… å¯ä»¥åˆ›å»ºGINç´¢å¼•åŠ é€ŸJSONæŸ¥è¯¢

#### 6.2 é€‰æ‹©æ€§ä¿å­˜rawå­—æ®µ

```python
@celery.task
def sync_cdrs_for_single_day(instance_id: int, date_str: str, save_raw=False):
    """
    åŒæ­¥è¯å•ï¼ˆå¯é€‰æ˜¯å¦ä¿å­˜åŸå§‹æ•°æ®ï¼‰
    
    Args:
        save_raw: æ˜¯å¦ä¿å­˜rawå­—æ®µï¼ˆé»˜è®¤FalseèŠ‚çœç©ºé—´ï¼‰
    """
    # ... åŒæ­¥é€»è¾‘
    
    newc = CDR(
        vos_id=inst.id,
        caller=caller,
        callee=callee,
        # ... å…¶ä»–å­—æ®µ
        raw=c if save_raw else None  # å¯é€‰ä¿å­˜raw
    )
```

---

### æ–¹æ¡ˆ7: æ—¶åºæ•°æ®åº“æ–¹æ¡ˆï¼ˆå¯é€‰ï¼‰â˜…â˜…â˜…

å¯¹äºè¶…å¤§è§„æ¨¡è¯å•ï¼Œå¯è€ƒè™‘ä½¿ç”¨ä¸“é—¨çš„æ—¶åºæ•°æ®åº“ï¼š

#### 7.1 TimescaleDBï¼ˆPostgreSQLæ‰©å±•ï¼‰

```sql
-- å®‰è£…TimescaleDBæ‰©å±•
CREATE EXTENSION IF NOT EXISTS timescaledb;

-- è½¬æ¢ä¸ºè¶…è¡¨ï¼ˆè‡ªåŠ¨åˆ†åŒºï¼‰
SELECT create_hypertable('cdrs', 'start_time');

-- è‡ªåŠ¨æ•°æ®å‹ç¼©
ALTER TABLE cdrs SET (
  timescaledb.compress,
  timescaledb.compress_segmentby = 'vos_id',
  timescaledb.compress_orderby = 'start_time DESC'
);

-- è‡ªåŠ¨å‹ç¼©ç­–ç•¥ï¼ˆ30å¤©å‰çš„æ•°æ®è‡ªåŠ¨å‹ç¼©ï¼‰
SELECT add_compression_policy('cdrs', INTERVAL '30 days');

-- æ•°æ®ä¿ç•™ç­–ç•¥ï¼ˆè‡ªåŠ¨åˆ é™¤1å¹´å‰çš„æ•°æ®ï¼‰
SELECT add_retention_policy('cdrs', INTERVAL '1 year');
```

**ä¼˜ç‚¹**:
- âœ… è‡ªåŠ¨åˆ†åŒºå’Œå‹ç¼©ï¼ˆå‹ç¼©æ¯”10:1ï¼‰
- âœ… æ—¶åºæŸ¥è¯¢æ€§èƒ½ä¼˜å¼‚
- âœ… å…¼å®¹PostgreSQLè¯­æ³•
- âœ… è‡ªåŠ¨æ•°æ®ä¿ç•™ç­–ç•¥

#### 7.2 ClickHouseï¼ˆä¸“ä¸šOLAPæ•°æ®åº“ï¼‰

é€‚åˆå¤§è§„æ¨¡åˆ†ææŸ¥è¯¢ï¼Œä½†éœ€è¦ç‹¬ç«‹éƒ¨ç½²ã€‚

---

## ğŸ“‹ å®æ–½è®¡åˆ’

### é˜¶æ®µ1: ç«‹å³å®æ–½ï¼ˆ1-2å¤©ï¼‰

- [x] æ·»åŠ æŸ¥è¯¢æ—¶é—´èŒƒå›´é™åˆ¶ï¼ˆæœ€å¤š31å¤©ï¼‰
- [x] æ·»åŠ ç»“æœé›†åˆ†é¡µ
- [x] é™åˆ¶æ–°å¢VOSèŠ‚ç‚¹é¦–æ¬¡åŒæ­¥èŒƒå›´ï¼ˆ7å¤©ï¼‰
- [x] æ·»åŠ æŸ¥è¯¢ç»“æœç¼“å­˜

### é˜¶æ®µ2: çŸ­æœŸä¼˜åŒ–ï¼ˆ1å‘¨ï¼‰

- [ ] å®æ–½æ•°æ®åº“åˆ†åŒºè¡¨ï¼ˆæŒ‰æœˆåˆ†åŒºï¼‰
- [ ] è¿ç§»ç°æœ‰æ•°æ®åˆ°åˆ†åŒºè¡¨
- [ ] åˆ›å»ºåˆ†åŒºç®¡ç†è„šæœ¬
- [ ] rawå­—æ®µæ”¹ä¸ºJSONB

### é˜¶æ®µ3: ä¸­æœŸä¼˜åŒ–ï¼ˆ2-4å‘¨ï¼‰

- [ ] å®æ–½æ•°æ®å½’æ¡£ç­–ç•¥
- [ ] å»ºç«‹å†·æ•°æ®å¯¹è±¡å­˜å‚¨
- [ ] ä¼˜åŒ–æ•°æ®åº“é…ç½®
- [ ] å»ºç«‹è‡ªåŠ¨ç»´æŠ¤ä»»åŠ¡

### é˜¶æ®µ4: é•¿æœŸè§„åˆ’ï¼ˆ3ä¸ªæœˆ+ï¼‰

- [ ] è¯„ä¼°TimescaleDBæ–¹æ¡ˆ
- [ ] å»ºç«‹æ•°æ®åˆ†æå¹³å°
- [ ] å®æ–½æ›´å¤šæ™ºèƒ½å½’æ¡£ç­–ç•¥

---

## ğŸ’° æˆæœ¬ä¸æ”¶ç›Šåˆ†æ

### ä¸ä¼˜åŒ–çš„ä»£ä»·

| æ•°æ®é‡ | æŸ¥è¯¢æ—¶é—´ | æ’å…¥é€Ÿåº¦ | å­˜å‚¨ç©ºé—´ |
|--------|----------|----------|----------|
| 1äº¿æ¡ | 10-30ç§’ | æ…¢ | 200GB |
| 5äº¿æ¡ | 1-5åˆ†é’Ÿ | å¾ˆæ…¢ | 1TB |
| 10äº¿æ¡ | è¶…æ—¶ | ææ…¢ | 2TB+ |

### ä¼˜åŒ–åæ•ˆæœ

| ä¼˜åŒ–æªæ–½ | æŸ¥è¯¢æå‡ | å­˜å‚¨èŠ‚çœ | å®æ–½éš¾åº¦ |
|----------|----------|----------|----------|
| åˆ†åŒºè¡¨ | 10-100å€ | 0% | ä¸­ |
| JSONB | 10-20% | 30-50% | ä½ |
| å½’æ¡£ç­–ç•¥ | - | 60-80% | ä¸­ |
| æ—¶é—´é™åˆ¶ | æ˜¾è‘— | - | ä½ |
| TimescaleDB | 100å€+ | 90% | é«˜ |

---

## ğŸ¯ æ¨èæ–¹æ¡ˆç»„åˆ

### æœ€å°å¯è¡Œæ–¹æ¡ˆï¼ˆMVPï¼‰
1. âœ… æŸ¥è¯¢æ—¶é—´èŒƒå›´é™åˆ¶ï¼ˆ31å¤©ï¼‰
2. âœ… ç»“æœé›†åˆ†é¡µ
3. âœ… é¦–æ¬¡åŒæ­¥é™åˆ¶ï¼ˆ7å¤©ï¼‰

### æ ‡å‡†æ–¹æ¡ˆï¼ˆæ¨èï¼‰
1. âœ… æœ€å°å¯è¡Œæ–¹æ¡ˆ
2. âœ… æ•°æ®åº“åˆ†åŒºè¡¨ï¼ˆæŒ‰æœˆï¼‰
3. âœ… rawå­—æ®µJSONBåŒ–
4. âœ… æ•°æ®å½’æ¡£ç­–ç•¥ï¼ˆ12ä¸ªæœˆï¼‰

### ä¼ä¸šæ–¹æ¡ˆï¼ˆæœ€ä¼˜ï¼‰
1. âœ… æ ‡å‡†æ–¹æ¡ˆ
2. âœ… TimescaleDBé›†æˆ
3. âœ… å¯¹è±¡å­˜å‚¨å½’æ¡£
4. âœ… åˆ†ææŸ¥è¯¢ç‹¬ç«‹é›†ç¾¤

---

## ğŸ“ ç›‘æ§æŒ‡æ ‡

```sql
-- è¡¨å¤§å°ç›‘æ§
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE tablename LIKE 'cdrs%'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- æŸ¥è¯¢æ€§èƒ½ç›‘æ§
SELECT 
    query,
    mean_exec_time,
    calls
FROM pg_stat_statements
WHERE query LIKE '%cdrs%'
ORDER BY mean_exec_time DESC
LIMIT 10;

-- ç´¢å¼•ä½¿ç”¨ç‡
SELECT 
    indexrelname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
WHERE schemaname = 'public' AND tablename LIKE 'cdrs%';
```

---

éœ€è¦æˆ‘å¸®æ‚¨å®æ–½å“ªä¸ªæ–¹æ¡ˆå—ï¼Ÿæˆ‘å»ºè®®ä»**æ ‡å‡†æ–¹æ¡ˆ**å¼€å§‹ï¼

